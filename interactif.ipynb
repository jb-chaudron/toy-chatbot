{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/blob/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "path_model = \"/path_to_the_model_mistral/mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "\n",
    "\n",
    "llm = LlamaCPP(model_url=model_url,\n",
    "                model_path=path_model,\n",
    "                model_kwargs={\"n_gpu_layers\": -1},\n",
    "                verbose=True,\n",
    "                temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index import Prompt\n",
    "\n",
    "path_vector_db = \"<vector_database>\"\n",
    "storage_context = StorageContext.from_defaults(persist_dir=path_vector_db)\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de l'engine de discussion\n",
    "## index.as_chat_engine() si tu veux un chatbot\n",
    "## index.as_query_engine() si tu veux qu'il réponde à tes questions un peu mieux mais sans interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le paramètre similarity_top_k donne le nombre de chunk de texte de ta base de données\n",
    "# Qui vont être utilisés ! \n",
    "template = (\n",
    "    \"Voici du context pour répondre à une question que je vais te poser \\n\"\n",
    "    \"{context_str} \\n\"\n",
    "    \"Réponds à {query_str} à partir des informations précédentes \\n\"\n",
    ")\n",
    "\n",
    "qa_template = Prompt(template)\n",
    "\n",
    "# chatbot = index.as_chat_engine()\n",
    "QA_engine = index.as_query_engine(verbose=True,\n",
    "                                  text_qa_template=qa_template,\n",
    "                                  similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat = chatbot.chat(\"bonjour j'ai une question\")\n",
    "response = QA_engine.query(\"bonjour j'ai une question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenant un script pour finetune ton embedding si jamais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantité de données générées\n",
    "N_TEXTS = 100\n",
    "\n",
    "# Si ça prend trop de temps réduit ça, si tu veux de meilleurs perf augmente\n",
    "N_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from question_answering_dataset_generator import random_texts, questions_generation\n",
    "from sentence_transformers import InputExample\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "import joblib\n",
    "from sentence_transformers import losses, SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "GPU_DEVICE = \"CUDA\" if torch.cuda.is_available else \"mps\" if torch.mps.is_available else \"cpu\"\n",
    "\n",
    "path_vector_db = \"/Users/jean-baptistechaudron/Documents/Uruk/<uruk_maquette_vicuna>\"\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=path_vector_db)\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "answers = random_texts(index, n_texts=N_TEXTS)\n",
    "dataset = questions_generation(answers, llm)\n",
    "\n",
    "all_data = list(dataset.items())\n",
    "train_index = random.sample(list(range(len(all_data))), k = int(0.7*len(all_data)))\n",
    "train_data = [all_data[i] for i in train_index]\n",
    "eval_data = [all_data[i] for i in range(len(all_data)) if not i in train_index]\n",
    "\n",
    "joblib.dump(train_data,\"training_qa_dataset.joblib\")\n",
    "joblib.dump(eval_data,\"evaluation_qa_dataset.joblib\")\n",
    "\n",
    "train_examples = []\n",
    "\n",
    "for (q,a) in train_data:\n",
    "    train_examples.append(InputExample(texts=[q,a]))\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "\n",
    "embedding_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1-finetuned\"\n",
    "model = SentenceTransformer(embedding_model)\n",
    "model.to(GPU_DEVICE)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=N_EPOCHS)\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour évaluer à quel point le nouveau modèle est performant par rapport à l'autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "model.eval()\n",
    "for (q,a) in tqdm(eval_data):\n",
    "    with torch.no_grad():\n",
    "        tokenized_q, tokenized_a = model.tokenize(q), model.tokenize(a)\n",
    "        tokenized_q = {key : item.to(GPU_DEVICE) for key, item in tokenized_q.items()}\n",
    "        tokenized_a = {key : item.to(GPU_DEVICE) for key, item in tokenized_a.items()}\n",
    "        embedding_q, embedding_a = model(tokenized_q), model(tokenized_a)\n",
    "        loss = torch.dot(embedding_q[\"sentence_embedding\"].mean(0),embedding_a[\"sentence_embedding\"].mean(0))\n",
    "        print(loss)\n",
    "        result.append(loss.detach().tolist())\n",
    "\n",
    "\n",
    "vanilla_embedding_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "model_vanilla = SentenceTransformer(embedding_model)\n",
    "model_vanilla.eval()\n",
    "results_vanilla = []\n",
    "for (q,a) in tqdm(eval_data):\n",
    "    with torch.no_grad():\n",
    "        tokenized_q, tokenized_a = model_vanilla.tokenize(q), model_vanilla.tokenize(a)\n",
    "        tokenized_q = {key : item.to(GPU_DEVICE) for key, item in tokenized_q.items()}\n",
    "        tokenized_a = {key : item.to(GPU_DEVICE) for key, item in tokenized_a.items()}\n",
    "        embedding_q, embedding_a = model_vanilla(tokenized_q), model_vanilla(tokenized_a)\n",
    "        loss = torch.dot(embedding_q[\"sentence_embedding\"].mean(0),embedding_a[\"sentence_embedding\"].mean(0))\n",
    "        print(loss)\n",
    "        results_vanilla.append(loss.detach().tolist())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
